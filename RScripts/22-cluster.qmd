# Cluster analysis

## kmeans

```{r}
pacman::p_load(conflicted, tidyverse,
               wrappedtools, 
               palmerpenguins,
               ggfortify, GGally,
               factoextra,
               caret,clue,
               dendextend, circlize,
               easystats, NbClust, mclust
)

# conflict_scout()
conflicts_prefer(dplyr::slice, 
                 dplyr::filter,
                 palmerpenguins::penguins)
rawdata <- penguins |> 
  na.omit()
rawdata <- mutate(rawdata,
                  ID=paste('P', 1:nrow(rawdata))) |> 
  select(ID, everything())
predvars <- ColSeeker(namepattern = c('_mm','_g'))

rawdata <- predict(preProcess(rawdata |> 
                               select(predvars$names),
                             method = c("center", "scale")),
                  rawdata)
fviz_nbclust(rawdata |> select(predvars$names),
             FUNcluster = kmeans)

kmeans_out <- kmeans(rawdata |> select(predvars$names),
                     centers = 2)

rawdata <- 
  mutate(rawdata,Cluster_k2var4=kmeans_out$cluster |> as.factor())
rawdata |> 
  ggplot(aes(species,fill=Cluster_k2var4))+
  geom_bar()

rawdata |> 
  ggplot(aes(fill=species,x=Cluster_k2var4))+
  geom_bar(position = 'fill')+
  scale_y_continuous(name = 'Frequency', labels=scales::percent)


rawdata |> 
  ggplot(aes(flipper_length_mm,bill_length_mm,
             shape=species,color=Cluster_k2var4))+
  geom_point()

rawdata |> 
  ggplot(aes(flipper_length_mm,bill_length_mm,
             shape=species,color=Cluster_k2var4))+
  geom_point()


fviz_cluster(kmeans_out,rawdata |> select(predvars$names))


# predict(kmeans_out)
sample(clue::cl_predict(kmeans_out),10)

# more clusters
kmeans_out3 <- kmeans(rawdata |> select(predvars$names),
                     centers = 3)

rawdata <- 
  mutate(rawdata,Cluster_k3var4=kmeans_out3$cluster |> as.factor())
rawdata |> 
  ggplot(aes(species,fill=Cluster_k3var4))+
  geom_bar()

rawdata |> 
  ggplot(aes(fill=species,x=Cluster_k3var4))+
  geom_bar(position = 'fill')+
  scale_y_continuous(name = 'Frequency', labels=scales::percent)


rawdata |> 
  ggplot(aes(flipper_length_mm,bill_length_mm,
             shape=species,color=Cluster_k3var4))+
  geom_point()

rawdata |> 
  ggplot(aes(flipper_length_mm,bill_length_mm,
             shape=species,color=Cluster_k3var4))+
  geom_point()



kmeans_out6 <- kmeans(rawdata |> select(predvars$names),
                     centers = 6)

rawdata <- 
  mutate(rawdata,Cluster_k6var4=kmeans_out6$cluster |> as.factor())
rawdata |> 
  ggplot(aes(species,fill=Cluster_k6var4))+
  geom_bar()+
  facet_grid(rows=vars(sex))

rawdata |> 
  ggplot(aes(fill=species,x=Cluster_k6var4))+
  geom_bar(position = 'fill')+
  scale_y_continuous(name = 'Frequency', labels=scales::percent)+
  facet_grid(rows=vars(sex))


rawdata |> 
  ggplot(aes(flipper_length_mm,bill_length_mm,
             shape=species,color=Cluster_k6var4))+
  geom_point()+
  facet_grid(rows=vars(sex))

rawdata |> 
  ggplot(aes(flipper_length_mm,bill_length_mm,
             shape=species,color=Cluster_k6var4))+
  geom_point()


```

## HClust

```{r}
penguins_scaled <- 
  rawdata |> 
  select(predvars$names)
# Compute Distance Matrix
# Hierarchical clustering starts by calculating the distance between every
#  pair of observations. The most common distance metric is Euclidean distance.

distance_matrix <- dist(penguins_scaled, method = "euclidean")

as.matrix(distance_matrix)[1:5, 1:5] # small portion of the matrix


summary(distance_matrix) 

# Perform Hierarchical Clustering
#    `hclust()` from base R.
#    `method` parameter specifies the agglomeration method (linkage method).
#    Common methods: "ward.D2", "complete", "average", "single".
#    "ward.D2" is often preferred as it tends to produce more balanced clusters.

hc_result <- hclust(distance_matrix, method = "ward.D2")

hc_result


# Visualize the Dendrogram using factoextra
#    The dendrogram is the primary output of hierarchical clustering.
#    It shows how observations are grouped together.
#    `fviz_dend()` from `factoextra` provides a beautiful and easy way to plot it.

fviz_dend(hc_result,
          k = 3, # Cut the dendrogram into 3 groups (you can try other numbers)
          cex = 0.5, # Adjust label size
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800"), # Custom colors for clusters
          # color_labels_by_k = TRUE, # Color labels by group
          rect = TRUE, # Draw a rectangle around clusters
          # rect_border = c("#2E9FDF", "#00AFBB", "#E7B800"),
          rect_fill = TRUE,
          main = "Hierarchical Clustering Dendrogram (Ward's Method)",
          sub = "Penguins Data",
          xlab = "Observations",
          ylab = "Height (Distance)")


fviz_dend(hc_result,
          k = 6, cex = 0.5, 
          rect = TRUE,
          rect_fill = TRUE,
          main = "Hierarchical Clustering Dendrogram (Ward's Method)",
          sub = "Penguins Data", #ignored??
          xlab = "Observations",
          ylab = "Height (Distance)")


dend <- as.dendrogram(hc_result)
dend <- color_branches(dend, k=3)
dendextend::circlize_dendrogram(dend,
                                facing = "outside")


# Cut the Dendrogram and Extract Clusters
#    To form distinct clusters, you "cut" the dendrogram at a certain height
#    or specify the desired number of clusters (k).

num_clusters <- 3 # Let's aim for 3 clusters, similar to the 3 penguin species

# Cut tree into k groups
clusters <- cutree(hc_result, k = num_clusters)

sample(clusters,10)

# Add cluster assignments back to the original (non-scaled) data for easier interpretation
rawdata <- rawdata |> 
  mutate(cluster = as.factor(clusters))

rawdata |> 
  ggplot(aes(species,fill=cluster))+
  geom_bar()
sample(clue::cl_predict(kmeans_out),10)
rawdata |> 
  ggplot(aes(fill=species,x=cluster))+
  geom_bar(position = 'fill')+
  scale_y_continuous(name = 'Frequency', labels=scales::percent)
sample(clue::cl_predict(kmeans_out),10)

rawdata |> 
  ggplot(aes(flipper_length_mm,bill_length_mm,
             shape=species,color=cluster))+
  geom_point()
sample(clue::cl_predict(kmeans_out),10)
rawdata |> 
  ggplot(aes(flipper_length_mm,bill_length_mm,
             shape=species,color=cluster))+
  geom_point()
sample(clue::cl_predict(kmeans_out),10)

confusionMatrix(rawdata$cluster,reference = rawdata$species |> 
                  as.numeric() |> factor(levels=c(1,3,2),
                                         labels=c(1,2,3)))
sample(clue::cl_predict(kmeans_out),10)
# Visualize the Clusters on a Scatter Plot using factoextra
#    `fviz_cluster()` helps to visualize the clusters formed on a scatter plot
#    (using PCA for dimensionality reduction if data has > 2 dimensions).

fviz_cluster(list(data = penguins_scaled, cluster = clusters),
             geom = "point",
             ellipse.type = "convex", # Draw convex hulls around clusters
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), # Use same colors as dendrogram
             main = paste0("Hierarchical Clustering - ", num_clusters, " Clusters (PCA)"),
             # xlab = "Principal Component 1",
             # ylab = "Principal Component 2",
             ggtheme = theme_minimal())
sample(clue::cl_predict(kmeans_out),10)



```

## Unified from easystats

```{r}
easycluster1 <- cluster_analysis(penguins_scaled,
                                 standardize = FALSE)
print(easycluster1)
sample(clue::cl_predict(kmeans_out),10)
plot(easycluster1)
sample(clue::cl_predict(kmeans_out),10)

easycluster2 <- cluster_analysis(penguins_scaled,
                                 standardize = FALSE,
                                 method = "hclust",
                                 n = 6)
print(easycluster2)
sample(clue::cl_predict(kmeans_out),10)
plot(easycluster2)
sample(clue::cl_predict(kmeans_out),10)

```
